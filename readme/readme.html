<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
	<title>EF Stats 0.1.2 &ndash; Documentation</title>
    <meta name="AUTHOR" content="Martin Wohlauer">
	<meta name="DESCRIPTION" content="Documentation of the EF Stats Anylsis Tool">
	<meta name="KEYWORDS" content="Star Trek Voyager Elite Force Statistics Analysis">
	<meta http-equiv="CONTENT-TYPE" content="text/HTML; charset=utf8">
	<meta name="language" content="en">
	<style>
	  code
	  {
		  background: #E7E7E7;
	  }
      body
      {
          max-width: 800;
      }
      td
      {
          vertical-align: top;
          padding-right: 5px;
      }
      code
      {
          font-size: 15;
      }
      .image
      {
          text-align: center;
      }
      .image img
      {
          max-width: 100%;
      }
	</style>
  </head>
  <body lang="en-US" dir="LTR">
    <h1>EF Stats 0.1.2 Documentation</h1>
    <h2 id="contents">Contents</h2>
    <ul>
      <li><a href="#contents">Contents</a></li>
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#goal">Goals of This Project</a></li>
      <li><a href="#usage">Using EF Stats</a></li>
      <li><a href="#method">Analysis Method</a></li>
      <li><a href="#results">Changes in results to be expected</a>
        <ul>
          <li><a href="#beatingtodeath">No Beating to Death</a></li>
          <li><a href="#intrinsicvalue">The Intrinsic Rating of a Player</a></li>
          <li><a href="#unequalconverging">Unequal Players Still Converge</a></li>
          <li><a href="#comeback">The Come Back Effect</a></li>
          <li><a href="#earlyeffects">Effects of Early Frags on the Eventual Score</a></li>
        </ul>
      </li>
      <li><a href="#reading">OK, But What Can I Read From This?</a>
    </ul>
    <h2 id="introduction">Introduction</h2>
    <p>In 2000 the publisher Activision released Raven Software's first person shooter &raquo;Star Trek: Voyager Elite Force&laquo; (short: EF 1). The basis for this game was the Quake 3 Arena engine. So this is a first person shooter. It is featuring a single player campaign and multiplayer. The latter can be played competitively, of course. All competitive games seem to have one side effect: Players seeking some sort of ranking, indicating how well they handle themselves in the game against others.</p>
    <p>By default, the game only has a score board turning up during to so-called intermission. That's the time between two maps playing. It only shows the results from the last match/map. As is very common with shooters, this score board sorts only by frags (=kills). The players with the highest amount of frags is placed first, the rest sorted by frags after him. Nowadays there are also games, that use the kills-deaths-ratio as a means for determining the players rank. Another way is the so-called efficiency (kills devided by the sum of kills and deaths, so to speak the efficiency of successfully resolving an encounter positively).</p>
    <p>All the mentioned approaches to the matter of ranking or rating a players skill have one problem: They do not really take into account, how good or bad a player <i>actually</i> is, in comparison to other players. Any kill counts just as much as any other. This is specifically interesting, when bots come into play, which renders simple kill-counts as a means for rating human players against each other pretty much useless. Also the kills-deaths-ratio or efficiency are still strongly affected by players, that mount up a huge number of kills against for example bots or weaker players, while in turn those player's/bot's scores are wrecked in the same fashion.</p>
    <p>This can even produce ridiculously high values, when the analysis is done including infinity values. One player killing 2 bots, never be killed and never be heard from again would have a proper K:D-ratio of infinity forever. Also a ratio of 100&#8239;% would be untoppable. Such a start is a problem. And it also implies another problem: Players with only very few frags that are placed very high but would not even remotely be ranked that good, if they played more rounds. They only benefit from a good start.</p>
    <p>This always was a rather frustrating aspect for me. In other, more professional games this problem is also well known. The main question is: How to rank a player <i>realisticly</i>? If climbing up the ladder by winning against obviously inferior opponents is a safe bet, it will be done. Or to put it another way: Should an inferior player not be rewarded by a lot more rank-up for a win, than the other way around, the favored player when fragging him?</p>
    <p>In chess and many other games there's the concept of the Elo score (named after its creator, <a href="https://en.wikipedia.org/wiki/Arpad_Elo">Arpad Elo</a>). It takes into consideration, that the probability of a player to win a match against a defined other player should also reflect on changes of his ranking/rating, not just the match result itself. So there are actually solutions, which are a lot better than the usual approaches.</p>
    <h2 id="goal">Goals of This Project</h2>
    <p>This is the starting point and goal for this project: Create a ranking based on an Elo like score for EF 1. The basis for the actual data from which the score is derived, are the server log files of EF 1 servers. Every frag that is done (and also every death) is recorded in the log files. Those log entries are noted in a technical fashion, that can automatically be scanned for and analysed. And this is exactly what the EF Stats are supposed to be doing. They take the log file, analyse it and export the results in various available formats. This allows to use this data for example on a website and display a more or less accurate rating/ranking of players. It should run on the major platforms, Windows, Linux and MacOS. The choice here is C#, in form of a Mono project. It is not supposed to be a GUI tool. The target is more a server set up, where the log files are created and can be automatically analysed. In order to have compatibility with standard sort orders, K:D-ratios and efficiencies are also available and can be used for sorting.</p>
    <h2 id="usage">Using EF Stats</h2>
    <p>EF Stats comes with a bunch of options. The minimal command line would look like this:</p>
    <p><code>efstats.exe --inname <i>filename</i></code> (on Windows) and:</p>
    <p><code>mono efstats.exe --inname <i>filename</i></code> (on Linux/MacOS/FreeBSD).</p>
    <p>This runs the analysis on the given file, excluding bots, excluding certain default names and prints the results as HTML to STDOUT (prints it to the console), including nothing aside from that. The sort order of the resulting scores is by Elo value. Players with less than 100 recorded encounters are taken into analysis but not reported in that output file.</p>
    <p>There is quite a number of options and parameters you can use on top of that, to influence contents and their presentation:</p>
    <table>
      <tr><th>Option/Parameter</th><th>Effect</th></tr>
      <tr><td><code>--debug</code></td><td>This prints additional information during stats analysis runs, useful for debugging. Warning: This can be quite extensive, depending on the size of the log file being analyzed. Do not use it in productive scenarios.</td></tr>
      <tr><td><code>--elodetails</code></td><td>Setting this switch makes EF Stats additionally print the Elo score dynamics (changes with every frag) to STDOUT.</td></tr>
      <tr><td><code>--eloreport</code></td><td>Setting this switch makes EF Stats additionally print an Elo score probability analysis to STDOUT.</td></tr>
      <tr><td><code>--help</code></td><td>Prints the help text and stops, regardless of the presence of other parameters or values.</td></tr>
      <tr><td><code>--inname</code></td><td>This defines the name of the input file containing the log data to be analyzed. This parameter is required.</td></tr>
      <tr><td><code>--minenc</code></td><td>This optional parameter defines the minimum number of encounters a player has to be part of (sum of fragging or being fragged) in order to be listed. Default is 100. This prevents players who just droped by for a few frags and were never heard from again from distorting results. Below a certain number of frags the scoring actually cannot be accurate enough for a reasonable ranking.</td></tr>
      <tr><td><code>--outformat</code></td><td>This optional parameter defines the output format. You can choose from <code>text</code>, <code>csv</code>, <code>json</code> and <code>html</code>. Default is <code>html</code>.</td></tr>
      <tr><td><code>--outname</code></td><td>This defines the name of the output file that will contain the analysis results. This parameter is optional. If it is not set, the data is written to STDOUT. Important: Existing files will be overwritten without asking!</td></tr>
      <tr><td><code>--rpe</code></td><td>The optional switch <b>R</b>eport <b>P</b>arse <b>E</b>rrors (=rpe) makes the analyzer report problems with the input log file. By default such problems are not reported.</td></tr>
      <tr><td><code>--sortorder</code></td><td>This optional parameter defines the value that is used for the ranking. You can choose from <code>elo</code>, <code>ratio</code> and <code>eff</code> (efficiency). Default is <code>elo</code>.</td></tr>
      <tr><td><code>--statssave</code></td><td>Use this parameter to provide a save file name. By default, this program analyses the data from the entire log file and writes the results to the output file (or STDOUT). If you require an analysis of a log file that is being continuously extended (e.g. the live log file of a running EF server) this filename is used to load the analysis results from the last analysis run, and continue where the last analysis run left off. After the analysis the save file will be updated with the now added new analysis data. If there exists no analysis file under this name already, it will be created after analysis and the log analysis starts from the beginning of the log file, first. If you use this feature, you <i>really</i> should consider setting <code>g_logsync = 1</code> in your EF server's config.</td></tr>
      <tr><td><code>--verbose</code></td><td>The optional switch verbose makes EF Stats print some more information during the analysis run.</td></tr>
      <tr><td><code>--withbots</code></td><td>By default no bot players are included in the analysis. Setting this switch includes them.</td></tr>
      <tr><td><code>--withunnamed</code></td><td>By default no players with the nick names &raquo;Redshirt&laquo;, &raquo;RedShirt&laquo; or &raquo;UnnamedPlayer&laquo; are included in the analysis. Those are the standard nick names that the game uses when the player did not change his name. This means, various players use this nick, which will distort the results. Setting this switch includes them anyways.</td></tr>
    </table>
    <h2 id="knownproblems">Known Problems</h2>
    <p>Up to now there is one problem known. When the error message <i>The type initializer for 'Newtonsoft.Json.JsonWriter' threw an exception.</i> appears, you need a newer version of Mono. If your Linux distribution does not provide it via their official pakage repositories, you can get it <a href="http://www.mono-project.com/download/stable/">from the Mono development team</a> directly.</p>
    <h2 id="method">Analysis Method</h2>
    <p>EF Stats is going through the log file line by line. There are actually two kind of lines, that are of importance: Lines that report a nick name change or (after a connection) a new player's nick being registered. These lines give you step by step a mapping of nick names to slot ID values. These IDs are required for reading the other important log line type, the kill report. They can be occupied by another player later (being reused) but as long as no change of a slot happens, the slot will be occupied by the same player.</p>
    <p>Kills on the other hand are not denoted reliably by only nick names and weapon strings. Luckily the logs also show the ID values of players and used weapons first. So by using the before mentioned ID mapping slot to nick name, you can accurately identify the player nicks. As the weapon IDs are static, the mapping is also constant. (Of course, identifying the actual player is not a simple matter, as everybody can choose whatever nickname is desired. So technically speacking, the EF Stats are actually rating nick names, not neccessarily physical players.)</p>
    <p>When a new player (not known by the current analysis run) joins the server, he is assigned an Elo score of 15&#8239;000 and all values of that player are set to zero. For each log entry reporting a kill, both involved players kills or deaths are incremented (depending on whether the player was the attacker or the victim), their weapons usage and endurance updated and the Elo score difference calculated by the following formular:</p>
    <p class="image"><img src="delta.png" alt="Elo score formular" height="5%"></p>
    <p>The constant values for it are as follows: <i>S</i><sub><small>v</small></sub> = 300 (&wedgeq; value scale) and <i>S</i><sub><small>d</small></sub> = 4&#8239;000 (&wedgeq; difference scale). Along with the starting value of 15&#8239;000 for each new player, those were derived empirically. The scaling allowes for a detailed enough rating (many steps) as well as an adequate score penalties/gains for each kill. The Elo scores <i>E</i><sub><small>a</small></sub> (attacker Elo score) and <i>E</i><sub><small>v</small></sub> (victim Elo score) are taken from each player's existing data. Depending on whether the attacker was already the favored player, the difference (&Delta; - <i>S</i><sub><small>v</small></sub>) is added to the attacker, or if the victim was the favored player, the value &Delta; is added to the attacker. The victim get's the same value subtracted from his rating, as the attacker got added. There are never fractions added or subtracted. Scores are rounded to integer. This also means, that below a difference of 0.5 points, no difference will be applied, as a number smaller than 0.5 will be rounded to 0, equalling no change of score at all.</p>
    <p>This formula explicitly requires the Elo scores of both players. So suicides are not actually meant to be applied in this fashion. As a matter of fact, these instances are depicted by the logs by an attacker with ID 1022, named &lt;world&gt;. On the long run, this pseudo player would drain all the Elo points from all players involved. So it is ignored with regards to Elo scores, but still counts as a death.</p>
    <p>This formula has some implications:</p>
    <ul>
      <li>The rating difference of both involved players scales the penalty/gain: The formula clearly shows, that the difference of both player's Elo scores actually determines the change in rating/rank. The change is biggest when the difference is considerably but the favored player lost. On the other hand it is 0 where the difference is considerably but the favored player won. Put simply, of a pair of players, the worse players get less of a penalty than better player for the frag. Or the other way around, the player with the better rating gains less rating than the worse player.</li>
      <li>Convergence for consecutive kills: If you let two player fight it out over and over again, and only one player wins, the maximum score difference, that makes an eventual score penalty, is 11&#8239;110. The maximum gain/penalty of 300 could be achieved in such a situation, when the unfavored player wins.</li>
      <li>Convergence around the actual Elo score of both players: If two players fight it out over and over again, after just enough frags the scores of both players should converge roughtly around their intrinsic rating. The value will fluctuate with each frag due to the statistical element, but the rough score area should not change too much.</li>
      <li>Conservativism of Elo scores: Aside from a new player joining the analysis (adding a default score of 15&#8239;000 to the pool), no scores are added to or removed from the whole. This automatically implies, the average score or all players is always 15&#8239;000.</li>
      <li>The order, in which frags occur, can have a significant influence on the eventual rating.</li>
    </ul>
    <h2 id="results">Changes in results to be expected</h2>
    <p>You might think, &raquo;why make all that effort?&laquo; After all, good players will appear in the upper range, bad players will be placed low. So what can I actually expect from this Elo approach? In order to make this a bit more understandable, here are some examples.</p>
    <h3 id="beatingtodeath">No Beating to Death</h3>
    <p>If you consider the above mentioned scenario, that one player beats the other over and over again, there will be the point where the actual achievements of the losing player won't make a difference for a very long time, if usual K:D-ratios are applied for a ranking. Number example: Both players have 100 frags, all won by Player 1 against Player 2, except for one. This gives a K:D-ratio of 99:1 (99) for Player 1, and 1:99 (0.01) for Player 2. The efficiency would be 0.99 for Player 1 and 0.01 for Player 2. Imagine a third Player joining, getting fragged 99 times by Player 2 and 1 time the other way around. So Player 2 is equally better than Player 3 as Player 1 is in comparison to Player 2. The results for Player 3 would end the same way, ratio 0.01 and efficiency 0.01. Player 2 on the other hand would end up with 100 kills, 100 deaths, with an efficiency of 0.5 and a ratio of 1. Still, after winning 100 rounds, Player 2 is in absolute numbers so far behind Player 1, that it is very unlikely that he might ever catch up with him.</p>
    <p>The elo score would slow that down a bit, up to the point, where no change is applied any longer. And that one win against Player 1 (or against Player 2 by Player 3) would increase the score a whole lot more, than normal ratio/efficiency calculations would do. That's actually a rather fair approach, as afterall, this success is certainly more significant, than the last win of Player 1 against Player 2, even be it only by some random chance. Or in other words, the king of the hill position has a risk of getting pulled down rather fast by other players, that are not that bad in comparison, giving a more realistic estimate of how good the player actually is.</p>
    <p>If you have a look at a similar scenario, the Elo score penatly will cut off at some point:</p>
    <p class="image"><img src="demo4.png" alt="limited penalty for Elo scores for to high skill differences" width="100%"></p>
    <p>After 1&#8239;530 consecutive frags, the penalty decreases to zero. This is due to the integer difference calculation. Once the penalty falls below 0.5, the resulting difference is zero. So essentially, rating-wise speaking, a player is not beaten to death. A vampirism-like score drain should not continue indefinitely in normal scenarios. You can also see the kinks in both curves at frags 575 and 352. This is when the corresponding values fall below 1.5 and 2.5, ending in penalties of -1 and -2.</p>
    <h3 id="intrinsicvalue">The Intrinsic Rating of a Player</h3>
    <p>An optimal rating system would accurately reflect the skill of a player by the given rating. This assumes, that there is such a thing. In reality, all player's performance may vary, depending on the situation on a particular day, on a particular map, with particular opponents or simply in a certain network situation. So the intrinsic skill is difficult to represent just by a single number. But a not too rough estimate of that intrinsic rating as a measure for skill can still be obtained, as the following graphs will show.</p>
    <p>Imagine two players of exactly equal skill, playing a match, where every time one frags the other, during the next encounter the other player frags him back. The Elo score would behave in such a fashion:</p>
    <p class="image"><img src="demo3.png" alt="alternating frags result in a semi-stable osczillation around the intrinsic value of two players"></p>
    <p>In this case, both start off at the standard start value 15&#8239;000. Player 2 draws first blood, but after only a few frags, both players reach a semi stable situation, where their respective scores only oszillate around the start value. There is still a tiny difference of 5 points between them, which is a remnant effect of Player 2 hitting first. (See section <a href="#earlyeffects">Effects of Early Frags on the Eventual Score</a> on the effect of early changes.) So score differences of 5 or less actually have no real meaning. But even the alternation of roughly 150 points each turn shows, a difference of 150 means not really much. It is not a random aspect, that this value is half of the value scale <i>S</i><sub><small>v</small></sub>. When looking at the formula, then both players having the same score leads to a difference of 150. If both players have the same rating, the difference is 150 points at first and this continues later on. So a difference of less than 150 points also does not really indicate a real skill difference.</p>
    <h3 id="unequalconverging">Unequal Players Still Converge</h3>
    <p>Now let's change that scenario a bit. Let's say, Player 1 has a 30&#8239;% chance to frag Player 2. The following diagram shows a random simulation of exaclty that assumption. 1&#8239;000 frags with that distribution gave this result:</p>
    <p class="image"><img src="demo6.png" alt="30% win chance vs. 70% win chance, Elo score of a random frag series of that distribution"></p>
    <p>As you can clearly see, the random aspect still makes it not always clear, which player is actually the better player. Around frag 425 there is even a situation, where the worse player seems to have a <a href="#comeback">come back</a>. But when looking at both graphs, they also have a clear tendency of alternating around an average value of round about 14&#8239;000 for Player 1 and 16&#8239;000 for Player 2. So a difference of roughly 1:2 in kill scores (here: 300 vs. 700) gives already a difference of 2&#8239;000 from each other. To make an even clearer example, here is the result of the same simulation, applying a 10&#8239;% vs. 90&#8239;% win chance:</p>
    <p class="image"><img src="demo7.png" alt="10% win chance vs. 90% win chance, Elo score of a random frag series of that distribution"></p>
    <p>Player 1 falls to about 13&#8239;000, while Player 2 rises up to around 17&#8239;000. That is already a difference of 4&#8239;000. Which is around the difference scale <i>S</i><sub><small>d</small></sub>. A frag likelyhood of 1:9 gives a score difference of the difference scale, which is significant. Also you can see, that the tendency to stay in the own range of each player is a lot greater than in the previous example. The variations seem to be a lot less. But even so, the scores range from approximately 18&#8239;000 to 16&#8239;000. So a variation of 2&#8239;000 seems to be quite normal, anyways.</p>
    <p>Now the 1 on 1 examples might look a bit too artificial. Let's have a look at a similar scenario, that involves more than one pair or players. This simulation worked as follows: Any encounter of a player to another player is equally likely to occur. (You cannot really choose which player you will encounter next in reality either.) But the probability is derived by two random numbers, limited by a maximum value defined for each player. The player with the higher random number wins the encounter. The values for the four players were chosen as follows:
    </p>
    <table>
      <tr><th>Player</th><th>Maximum Random Number</th></tr>
      <tr><td>Player 1</td><td>10</td></tr>
      <tr><td>Player 2</td><td>25</td></tr>
      <tr><td>Player 3</td><td>60</td></tr>
      <tr><td>Player 4</td><td>90</td></tr>
    </table>
    <p class="image"><img src="demo8.png" alt="1000 random encounters of four players with indiviual intrinsic skill scores between 10 and 90."></p>
    <p>Interestingly, Player 1 (value 90) and Player 4 (value 10) seem to share a similarity in scores in comparison with the 10&#8239;%/90&#8239;% simulation: Player 1 ranges around 17&#8239;750, which is close to the 17&#8239;000 when only those two players are involved. Player 4 on the other hand ends up around 11&#8239;750, which again is not too far away from the previous 13&#8239;000. But apparently having a lot more of a chance again Players 2 and 3 already increases the value by about 1&#8239;000 points. And the other way around, having more of a competition, Player 4's score does not simply explode by more targets. This is exactly the desired effect: Losing against a very good player is not as much as a big deal for the score, than the other way around. An average player, that might loose any time against a good player still won't be reduced into oblivion, but has a strong chance of making a decent come back when playing also against less or similarly strong opponents. So the high skill of one player does not dominate all the other players that much, as they tend to stabilize each other and are also favored by penalty due to their under dog position. The fact, that Player 3 (value 60) in comparison to Player 2 (value 25) reaches a similar situation as Player 1 and 2 had in the previous 30&#8239;%/70&#8239;% scenario (similar difference) points into the direction, that an intrinsic value (here: the chosen numbers) is having an effect on the qualitative results. It seems to give a similar difference (14&#8239;700 to 16&#8239;200) as the 30&#8239;%/70&#8239;% example did before (14&#8239;000 vs. 16&#8239;000). So despite the obvious random influence, the effect of intrinsic skill values can clearly be seen. But the variance is quite high. So an absolute Elo score will only give an estimate of the actual skill, although it seems unlikely to vary significantly for a certain player.</p>
    <h3 id="comeback">The Come Back Effect</h3>
    <p>As the unfavored player in a match is still a bit favored in terms of less penalty and more gain for a frag, there is one particular effect, that can strike especially strong in the very early history of a player:</p>
    <p class="image"><img src="demo5.png" alt="Player winning 10 frags and then losing 10 frags against the same opponent results in less than the starting value for the formerly favored player"></p>
    <p>In this graph you can clearly see, how the winner is gaining less and less points from fragging the nominally unfavored player. During the first 10 frags, the score of Player 1 is clearly being drained, but less and less, as he will be estimated less and less likely to win an encounter against this opponent. In the same fashion can you see, that an unfavored player get's a lot more gain than the favored player. This can lead to the shown come back effect: A player losing a lot in the beginning will eventually have an ever higher score than the formerly favoured player, even when both have an equal number of frags against each other. Therefore it is important, to not overestimate early successes or failures. As the other graphs before also show, before the first 150 to 200 frags happened, it is difficult to make a proper assessment of a player's rating. One side effect of this is also: If a player actually gains in experience and skill, he has it a lot more easy to also increase his Elo score on the expense of formerly (or still) better players. So the score is rather sensitive against actual skill changes. This is in strong contrast to the classic K:D ratio or efficiency, which might not change very much by themselves. And on top, might not change anything in ranking, as other's are simply too far away and the history of the player still kills off the effect of his current successes. So while the early results might have an effect that can easily be overestimated, the later results can also be of strong significance, if the player gains skill.</p>
    <h3 id="earlyeffects">Effects of Early Frags on the Eventual Score</h3>
    <p>To investigate this specific aspect, that the time of a frag occuring can have a significant difference in score as a result, let's have a look the the following two frag series:</p>
    <p class="image"><img src="demo1.png" alt="Three players, Player 2 fragging the other ten times each, Player 3 fragging 2 eventually one time"></p>
    <p>Here you can see the results of a simulated run: Player 2 wins 10 encounters against Player 1 and then another 10 against Player 3. And eventually Player 3 strikes back and frags Player 1 one time.</p>
    <p class="image"><img src="demo2.png" alt="Three players, Player 2 fragging the other ten times each, Player 3 fragging 2 first one time"></p>
    <p>This is the opposite situation: Player 3 frags Player 2 one time, then Player 1 get's his 10 deaths and another 10 for Player 3. The results of both series' eventual scores are shown here:</p>
    <table>
      <thead>Late Revenge: 1 last frag</thead>
      <tr><th>Rank</th><th>Player</th><th>Elo</th><th>Efficiency</th><th>Ratio</th><th>Score</th><th>Eliminated</th></tr>
      <tr><td>1</td><td>Player 2</td><td>16&#8239;590</td><td>0.95</td><td>20.00</td><td>20</td><td>1</td></tr>
      <tr><td>2</td><td>Player 3</td><td>14&#8239;466</td><td>0.09</td><td>0.10</td><td>1</td><td>10</td></tr>
      <tr><td>3</td><td>Player 1</td><td>13&#8239;944</td><td>0.00</td><td>0.00</td><td>0</td><td>10</td></tr>
    </table>
    <table>
      <thead>The early strike: 1 first frag</thead>
      <tr><th>Rank</th><th>Player</th><th>Elo</th><th>Efficiency</th><th>Ratio</th><th>Score</th><th>Eliminated</th></tr>
      <tr><td>1</td><td>Player 2</td><td>16&#8239;793</td><td>0.95</td><td>20.00</td><td>20</td><td>1</td></tr>
      <tr><td>2</td><td>Player 3</td><td>14&#8239;305</td><td>0.09</td><td>0.10</td><td>1</td><td>10</td></tr>
      <tr><td>3</td><td>Player 1</td><td>13&#8239;902</td><td>0.00</td><td>0.00</td><td>0</td><td>10</td></tr>
    </table>
    <p>When you compare the two tables, you see easily, that they differ not by much. Only the Elo score seems to vary. You can see here the effect, that great score differences make. That last kill in case 1 (late kill) packs a lot more punch, than when it happened very early on. So the order of frags can really make a significant difference. When you look at the other examples, you will also see, that this effect is strong, especially for the 10&#8239;%/90&#8239;% example. The increase of score for the less favored player is visibly higher than for the favoured player. This really shows, why the <code>--minenc</code> parameter may be important and why there is an initial filter for at least 100 frags for each player. It simply does not say much, if the player barely had any chance to show his real skill.</p>
    <p>The underlying problem is, that we cannot give an estimate for the rating right from the start. In chess for example, you play some trial matches, just to see how well you are doing. And instead of slowly climbing up a ladder system, you are placed on a roughly fitting spot early on. As we do not know anything about a newly appearing nick name/player, we don't have that luxury. So one of the main weaknesses here is the generic starting value of 15&#8239;000. After a while it won't make a difference any more, but for early results it can make a significant difference, what happens there. After roughly 200 frags the true skill of a player gets rather evident. Below 100 it is actually to soon to make an assessment. So the question might be: Why is the default filter for player frags 100? Why not at least 200? That's a concession to the fact, that new players might lose interest quickly, if they play, and play, and never show up on the score board, as trying to reach 200 frags can take a while, depending on the server settings. Classic 20 frags map rounds in FFA take at least 10 maps to even appear in the ranking, assuming the new player won every round. That can be an entire EF evening, depending on played maps, number of players active and skill of the other players.
    <h2 id="reading">OK, But What Can I Read From This?</h2>
    <p>To be honest, it's still only a tendency. How a certain encounter of players during a match will end, always retains a random element. What health do both players have? Maybe one of them just caught the other pants down. What items do they carry? It's an up-hill battle to frag someone who is boosted by the quantum enhancer and has 200 shields and the Nano-regenerative shield. Are there any map advantages for either side? Sniping from far above is a lot more fun when you are the sniper...</p>
    <p>But the Elo score is surely a way of estimating the skill of players, because on the average those random aspects should cancel each other out. The systematical tendencies (e.g. is the player able to make use of the map items, how well does he aim, is his movement good enough to not make an easy target of himself, does he know how to use the map layout and weapon peculiarities to his advantage, etc.) however, will essentially turn up as a different score value, the intrinsic value. But a single encounter can only be predicted with a bigger random aspect to it. Here is a graph showing the probability for a successfull frag in comparison to the Elo score difference of the involved players (just to give you an Idea what this means):</p>
    <p class="image"><img src="probabilities.png" alt="Winning probability in comparison to the Elo score difference"></p>
    <p>This graph's data is taken from an actual long time server log (approximately 9&#8239;000 encounters taken), including bots and unnamed players, but excluding difference occurances which happend less than 5 times. Not surprising, for two players of equal score the probability to win is 50:50, according to the derived linear function. But you can also see, that there is considerable variance from it present. The correlation coefficient of 0.639 clearly shows, that there is a big amount of deviation from that simple linear equation. But the tendency clearly is linear, with some artifacts around 0&#8239;%/100&#8239;%. For a rough estimate, the range around &plusmn;2&#8239;000 around the line seems to be a rather safe zone.</p>
    <p>So when considering the magic number of the difference scale <i>S</i><sub><small>d</small></sub> (= 4&#8239;000) as a measure, then chances of success for both sides vary round about 85&#8239;% for the favored player but can stretch up to around 67&#8239;% to 100&#8239;%. When we compare that with the results from the 10&#8239;%/90&#8239;% tests, then it seems that the real data reduced to the formula seems to fit the simulation values. When you take the 30&#8239;%/70&#8239;% test, the Score difference calculated back via the formula gives you a score difference of roughly 2&#8239;300, which means for starting values of 15&#8239;000 the values 16&#8239;150 and 13&#8239;850. The estimates here seem to fit rather nicely to the simulated ones of 14&#8239;000 and 16&#8239;000.</p>
    <p>So real players in a far more complex situation give similar results as the simulations, although they are a lot more noisy when plotted. So one can assume that the formula <b>0.0086&#8239;% x Score difference + 50&#8239;%</b> is a rather good estimate of winning chances.
  </body>
</html>
